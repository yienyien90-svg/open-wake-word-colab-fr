{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yienyien90-svg/open-wake-word-colab-fr/blob/main/automatic_model_training_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training your own openWakeWord models\n"
      ],
      "metadata": {
        "id": "vl_FIEj-auGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick-start:** If you just want to train a basic custom model for openWakeWord!\n",
        "\n",
        "Follow the instructions for Step 1 below. Each time you change the wake word, click the play icon to the left of the title to generate a sample and make sure it sounds correct. The first time it takes a few minutes but subsequent runs will be quick.\n",
        "\n",
        "Once you're satisfied with the pronounciation, go to the \"Runtime\" dropdown menu in the upper left of the page, and select \"run all\". Keep the tab open but feel free to do something else. After ~1 hour, your custom model will be ready and will automatically be downloaded to your computer!\n",
        "\n",
        "If you are a Home Assistant user with the openWakeWord add-on, follow the instructions [here](https://github.com/home-assistant/addons/blob/master/openwakeword/DOCS.md#custom-wake-word-models) to install and enable your custom model.\n",
        "\n",
        "---\n",
        "\n",
        "If you are interested in learning more about the custom model training process (and increasing the accuracy of your custom models), read through each step in this notebook and try experimenting with different training parameters. If you have any questions or problems, feel free to start a discussion at the openWakeWord [repo](https://github.com/dscripka/openWakeWord/discussions)."
      ],
      "metadata": {
        "id": "-Q9wEuRdwY_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 1. Test Example Training Clip Generation\n",
        "# @markdown Since openWakeWord models are trained on synthetic examples of your\n",
        "# @markdown target wake word, it's a good idea to make sure that the examples\n",
        "# @markdown sound correct. Type in your target wake word below, and run the\n",
        "# @markdown cell to listen to it.\n",
        "# @markdown\n",
        "# @markdown Here are some tips that can help get the wake word to sound right:\n",
        "\n",
        "# @markdown - If your wake word isn't being pronounced in the way\n",
        "# @markdown you want, try spelling out the sounds phonetically with underscores\n",
        "# @markdown separating each part.\n",
        "# @markdown For example: \"hey siri\" --> \"hey_seer_e\".\n",
        "\n",
        "# @markdown - Spell out numbers (\"2\" --> \"two\")\n",
        "\n",
        "# @markdown - Avoid all punctuation except for \"?\" and \"!\", and remove unicode characters\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "    !wget -O piper-sample-generator/models/fr_FR-upmc-medium.onnx 'https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/upmc/medium/fr_FR-upmc-medium.onnx'\n",
        "    !wget -O piper-sample-generator/models/fr_FR-upmc-medium.onnx.json 'https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/upmc/medium/fr_FR-upmc-medium.onnx.json'\n",
        "    !cd piper-sample-generator\n",
        "\n",
        "    # Install system dependencies\n",
        "    !pip install piper-tts piper-phonemize-cross\n",
        "    !pip install webrtcvad\n",
        "    !pip install 'torch<=2.5' torchvision torchaudio\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "    sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "import generate_samples\n",
        "\n",
        "target_word = 'nexe-suss' # @param {type:\"string\"}\n",
        "\n",
        "def text_to_speech(text):\n",
        "    generate_samples.generate_samples_onnx(text = text,\n",
        "                max_samples=1,\n",
        "                length_scales=[1.1],\n",
        "                noise_scales=[0.7], noise_scale_ws = [0.7],\n",
        "                output_dir = './', batch_size=1, auto_reduce_batch_size=True,\n",
        "                file_names=[\"test_generation.wav\"],\n",
        "                model=\"piper-sample-generator/models/fr_FR-upmc-medium.onnx\"\n",
        "                )\n",
        "\n",
        "text_to_speech(target_word)\n",
        "Audio(\"test_generation.wav\", autoplay=True)"
      ],
      "metadata": {
        "id": "1cbqBebHXjFD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "34e169c3-67d3-45d8-b844-e695184793a9",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 2. Download Data\n",
        "# @markdown Training custom models requires downloading a wide variety of data\n",
        "# @markdown that will help make the model perform well in real-world scenarios.\n",
        "# @markdown This example notebook will download small samples of background noise,\n",
        "# @markdown music, and Room Impulse Responses (to add echo). This will still produce\n",
        "# @markdown a custom model that performs well, but if you are interested in adding even more,\n",
        "# @markdown feel free to extend this notebook to download the full datasets and even add\n",
        "# @markdown your own!\n",
        "# @markdown\n",
        "# @markdown Downloading this example data will usually take about 15 minutes.\n",
        "\n",
        "# @markdown **Important note!** The data downloaded here has a mixture of different\n",
        "# @markdown licenses and usage restrictions. As such, any custom models trained with this\n",
        "# @markdown data should be considered as appropriate for **non-commercial** personal use only.\n",
        "\n",
        "# ## Install all dependencies\n",
        "# !pip install datasets\n",
        "# !pip install scipy\n",
        "# !pip install tqdm\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# install openwakeword (full installation to support training)\n",
        "!git clone https://github.com/dscripka/openwakeword\n",
        "!pip install -e ./openwakeword --no-deps\n",
        "# !cd openwakeword\n",
        "!pip install tensorflow==2.19.0\n",
        "!pip install onnx2tf\n",
        "!pip install onnx==1.17.0\n",
        "!pip install onnxruntime==1.18.1\n",
        "# install other dependencies\n",
        "!pip install mutagen==1.47.0\n",
        "!pip install torchinfo==1.8.0\n",
        "!pip install torchmetrics==1.2.0\n",
        "!pip install speechbrain==0.5.14\n",
        "!pip install audiomentations==0.33.0\n",
        "!pip install torch-audiomentations==0.11.0\n",
        "!pip install acoustics==0.2.6\n",
        "# !pip uninstall tensorflow -y\n",
        "# !pip install tensorflow-cpu==2.8.1\n",
        "# !pip install protobuf==3.20.3\n",
        "# !pip install tensorflow_probability==0.16.0\n",
        "# !pip install onnx_tf==1.10.0\n",
        "!pip install ai_edge_litert==1.4.0 onnxsim\n",
        "# !pip install ai_edge_litert==1.2.0\n",
        "!pip install onnx_graphsurgeon\n",
        "!pip install sng4onnx\n",
        "!pip install pronouncing==0.2.0\n",
        "!pip install datasets==2.14.6\n",
        "!pip install deep-phonemizer==0.0.19\n",
        "\n",
        "# Download required models (workaround for Colab)\n",
        "import os\n",
        "os.makedirs(\"./openwakeword/openwakeword/resources/models\", exist_ok=True)\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite\n",
        "\n",
        "# Imports\n",
        "import sys\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "    sys.path.append(\"piper-sample-generator/\")\n",
        "from generate_samples import generate_samples\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import yaml\n",
        "import datasets\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Download all data\n",
        "\n",
        "## Download MIR RIR data (takes about ~2 minutes)\n",
        "output_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses\n",
        "    rir_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"./MIT_environmental_impulse_responses/16khz\").glob(\"*.wav\")]}).cast_column(\"audio\", datasets.Audio())\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "## Download noise and background audio (takes about ~3 minutes)\n",
        "\n",
        "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "\n",
        "if not os.path.exists(\"audioset\"):\n",
        "    os.mkdir(\"audioset\")\n",
        "\n",
        "    fname = \"bal_train09.tar\"\n",
        "    out_dir = f\"audioset/{fname}\"\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd audioset && tar -xvf bal_train09.tar\n",
        "\n",
        "    output_dir = \"./audioset_16k\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
        "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(audioset_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "\n",
        "output_dir = \"./fma\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
        "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
        "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
        "        row = next(fma_dataset)\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "        i += 1\n",
        "        if i == n_hours*3600//30:\n",
        "            break\n",
        "\n",
        "# Download pre-computed openWakeWord features for training and validation\n",
        "\n",
        "# training set (~2,000 hours from the ACAV100M Dataset)\n",
        "# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information\n",
        "if not os.path.exists(\"./openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"):\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
        "\n",
        "# validation set for false positive rate estimation (~11 hours)\n",
        "if not os.path.exists(\"validation_set_features.npy\"):\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
        "\n",
        "#Replace openwakeword train.py with a french version\n",
        "custom_train_script = '''\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torchinfo\n",
        "import torchmetrics\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import uuid\n",
        "import numpy as np\n",
        "import scipy\n",
        "import collections\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import openwakeword\n",
        "from openwakeword.data import generate_adversarial_texts, augment_clips, mmap_batch_generator\n",
        "from openwakeword.utils import compute_features_from_generator\n",
        "from openwakeword.utils import AudioFeatures\n",
        "\n",
        "\n",
        "# Base model class for an openwakeword model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_classes=1, input_shape=(16, 96), model_type=\"dnn\",\n",
        "                 layer_dim=128, n_blocks=1, seconds_per_example=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store inputs as attributes\n",
        "        self.n_classes = n_classes\n",
        "        self.input_shape = input_shape\n",
        "        self.seconds_per_example = seconds_per_example\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.best_models = []\n",
        "        self.best_model_scores = []\n",
        "        self.best_val_fp = 1000\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_val_recall = 0\n",
        "        self.best_train_recall = 0\n",
        "\n",
        "        # Define model (currently on fully-connected network supported)\n",
        "        if model_type == \"dnn\":\n",
        "            # self.model = nn.Sequential(\n",
        "            #     nn.Flatten(),\n",
        "            #     nn.Linear(input_shape[0]*input_shape[1], layer_dim),\n",
        "            #     nn.LayerNorm(layer_dim),\n",
        "            #     nn.ReLU(),\n",
        "            #     nn.Linear(layer_dim, layer_dim),\n",
        "            #     nn.LayerNorm(layer_dim),\n",
        "            #     nn.ReLU(),\n",
        "            #     nn.Linear(layer_dim, n_classes),\n",
        "            #     nn.Sigmoid() if n_classes == 1 else nn.ReLU(),\n",
        "            # )\n",
        "\n",
        "            class FCNBlock(nn.Module):\n",
        "                def __init__(self, layer_dim):\n",
        "                    super().__init__()\n",
        "                    self.fcn_layer = nn.Linear(layer_dim, layer_dim)\n",
        "                    self.relu = nn.ReLU()\n",
        "                    self.layer_norm = nn.LayerNorm(layer_dim)\n",
        "\n",
        "                def forward(self, x):\n",
        "                    return self.relu(self.layer_norm(self.fcn_layer(x)))\n",
        "\n",
        "            class Net(nn.Module):\n",
        "                def __init__(self, input_shape, layer_dim, n_blocks=1, n_classes=1):\n",
        "                    super().__init__()\n",
        "                    self.flatten = nn.Flatten()\n",
        "                    self.layer1 = nn.Linear(input_shape[0]*input_shape[1], layer_dim)\n",
        "                    self.relu1 = nn.ReLU()\n",
        "                    self.layernorm1 = nn.LayerNorm(layer_dim)\n",
        "                    self.blocks = nn.ModuleList([FCNBlock(layer_dim) for i in range(n_blocks)])\n",
        "                    self.last_layer = nn.Linear(layer_dim, n_classes)\n",
        "                    self.last_act = nn.Sigmoid() if n_classes == 1 else nn.ReLU()\n",
        "\n",
        "                def forward(self, x):\n",
        "                    x = self.relu1(self.layernorm1(self.layer1(self.flatten(x))))\n",
        "                    for block in self.blocks:\n",
        "                        x = block(x)\n",
        "                    x = self.last_act(self.last_layer(x))\n",
        "                    return x\n",
        "            self.model = Net(input_shape, layer_dim, n_blocks=n_blocks, n_classes=n_classes)\n",
        "        elif model_type == \"rnn\":\n",
        "            class Net(nn.Module):\n",
        "                def __init__(self, input_shape, n_classes=1):\n",
        "                    super().__init__()\n",
        "                    self.layer1 = nn.LSTM(input_shape[-1], 64, num_layers=2, bidirectional=True,\n",
        "                                          batch_first=True, dropout=0.0)\n",
        "                    self.layer2 = nn.Linear(64*2, n_classes)\n",
        "                    self.layer3 = nn.Sigmoid() if n_classes == 1 else nn.ReLU()\n",
        "\n",
        "                def forward(self, x):\n",
        "                    out, h = self.layer1(x)\n",
        "                    return self.layer3(self.layer2(out[:, -1]))\n",
        "            self.model = Net(input_shape, n_classes)\n",
        "\n",
        "        # Define metrics\n",
        "        if n_classes == 1:\n",
        "            self.fp = lambda pred, y: (y-pred <= -0.5).sum()\n",
        "            self.recall = torchmetrics.Recall(task='binary')\n",
        "            self.accuracy = torchmetrics.Accuracy(task='binary')\n",
        "        else:\n",
        "            def multiclass_fp(p, y, threshold=0.5):\n",
        "                probs = torch.nn.functional.softmax(p, dim=1)\n",
        "                neg_ndcs = y == 0\n",
        "                fp = (probs[neg_ndcs].argmax(axis=1) != 0 & (probs[neg_ndcs].max(axis=1)[0] > threshold)).sum()\n",
        "                return fp\n",
        "\n",
        "            def positive_class_recall(p, y, negative_class_label=0, threshold=0.5):\n",
        "                probs = torch.nn.functional.softmax(p, dim=1)\n",
        "                pos_ndcs = y != 0\n",
        "                rcll = (probs[pos_ndcs].argmax(axis=1) > 0\n",
        "                        & (probs[pos_ndcs].max(axis=1)[0] >= threshold)).sum()/pos_ndcs.sum()\n",
        "                return rcll\n",
        "\n",
        "            def positive_class_accuracy(p, y, negative_class_label=0):\n",
        "                probs = torch.nn.functional.softmax(p, dim=1)\n",
        "                pos_preds = probs.argmax(axis=1) != negative_class_label\n",
        "                acc = (probs[pos_preds].argmax(axis=1) == y[pos_preds]).sum()/pos_preds.sum()\n",
        "                return acc\n",
        "\n",
        "            self.fp = multiclass_fp\n",
        "            self.acc = positive_class_accuracy\n",
        "            self.recall = positive_class_recall\n",
        "\n",
        "        self.n_fp = 0\n",
        "        self.val_fp = 0\n",
        "\n",
        "        # Define logging dict (in-memory)\n",
        "        self.history = collections.defaultdict(list)\n",
        "\n",
        "        # Define optimizer and loss\n",
        "        self.loss = torch.nn.functional.binary_cross_entropy if n_classes == 1 else nn.functional.cross_entropy\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    def save_model(self, output_path):\n",
        "        \"\"\"\n",
        "        Saves the weights of a trained Pytorch model\n",
        "        \"\"\"\n",
        "        if self.n_classes == 1:\n",
        "            torch.save(self.model, output_path)\n",
        "\n",
        "    def export_to_onnx(self, output_path, class_mapping=\"\"):\n",
        "        obj = self\n",
        "        # Make simple model for export based on model structure\n",
        "        if self.n_classes == 1:\n",
        "            # Save ONNX model\n",
        "            torch.onnx.export(self.model.to(\"cpu\"), torch.rand(self.input_shape)[None, ], output_path,\n",
        "                              output_names=[class_mapping])\n",
        "\n",
        "        elif self.n_classes >= 1:\n",
        "            class M(nn.Module):\n",
        "                def __init__(self):\n",
        "                    super().__init__()\n",
        "\n",
        "                    # Define model\n",
        "                    self.model = obj.model.to(\"cpu\")\n",
        "\n",
        "                def forward(self, x):\n",
        "                    return torch.nn.functional.softmax(self.model(x), dim=1)\n",
        "\n",
        "            # Save ONNX model\n",
        "            torch.onnx.export(M(), torch.rand(self.input_shape)[None, ], output_path,\n",
        "                              output_names=[class_mapping])\n",
        "\n",
        "    def lr_warmup_cosine_decay(self,\n",
        "                               global_step,\n",
        "                               warmup_steps=0,\n",
        "                               hold=0,\n",
        "                               total_steps=0,\n",
        "                               start_lr=0.0,\n",
        "                               target_lr=1e-3\n",
        "                               ):\n",
        "        # Cosine decay\n",
        "        learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold)\n",
        "                                           / float(total_steps - warmup_steps - hold)))\n",
        "\n",
        "        # Target LR * progress of warmup (=1 at the final warmup step)\n",
        "        warmup_lr = target_lr * (global_step / warmup_steps)\n",
        "\n",
        "        # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether\n",
        "        # `global_step < warmup_steps` and we're still holding.\n",
        "        # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
        "        if hold > 0:\n",
        "            learning_rate = np.where(global_step > warmup_steps + hold,\n",
        "                                     learning_rate, target_lr)\n",
        "\n",
        "        learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
        "        return learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def summary(self):\n",
        "        return torchinfo.summary(self.model, input_size=(1,) + self.input_shape, device='cpu')\n",
        "\n",
        "    def average_models(self, models=None):\n",
        "        \"\"\"Averages the weights of the provided models together to make a new model\"\"\"\n",
        "\n",
        "        if models is None:\n",
        "            models = self.best_models\n",
        "\n",
        "        # Clone a model from the list as the base for the averaged model\n",
        "        averaged_model = copy.deepcopy(models[0])\n",
        "        averaged_model_dict = averaged_model.state_dict()\n",
        "\n",
        "        # Initialize a running total of the weights\n",
        "        for key in averaged_model_dict:\n",
        "            averaged_model_dict[key] *= 0  # set to 0\n",
        "\n",
        "        for model in models:\n",
        "            model_dict = model.state_dict()\n",
        "            for key, value in model_dict.items():\n",
        "                averaged_model_dict[key] += value\n",
        "\n",
        "        for key in averaged_model_dict:\n",
        "            averaged_model_dict[key] /= len(models)\n",
        "\n",
        "        # Load the averaged weights into the model\n",
        "        averaged_model.load_state_dict(averaged_model_dict)\n",
        "\n",
        "        return averaged_model\n",
        "\n",
        "    def _select_best_model(self, false_positive_validate_data, val_set_hrs=11.3, max_fp_per_hour=0.5, min_recall=0.20):\n",
        "        \"\"\"\n",
        "        Select the top model based on the false positive rate on the validation data\n",
        "\n",
        "        Args:\n",
        "            false_positive_validate_data (torch.DataLoader): A dataloader with validation data\n",
        "            n (int): The number of models to select\n",
        "\n",
        "        Returns:\n",
        "            list: A list of the top n models\n",
        "        \"\"\"\n",
        "        # Get false positive rates for each model\n",
        "        false_positive_rates = [0]*len(self.best_models)\n",
        "        for batch in false_positive_validate_data:\n",
        "            x_val, y_val = batch[0].to(self.device), batch[1].to(self.device)\n",
        "            for mdl_ndx, model in tqdm(enumerate(self.best_models), total=len(self.best_models),\n",
        "                                       desc=\"Find best checkpoints by false positive rate\"):\n",
        "                with torch.no_grad():\n",
        "                    val_ps = model(x_val)\n",
        "                    false_positive_rates[mdl_ndx] = false_positive_rates[mdl_ndx] + self.fp(val_ps, y_val[..., None]).detach().cpu().numpy()\n",
        "        false_positive_rates = [fp/val_set_hrs for fp in false_positive_rates]\n",
        "\n",
        "        candidate_model_ndx = [ndx for ndx, fp in enumerate(false_positive_rates) if fp <= max_fp_per_hour]\n",
        "        candidate_model_recall = [self.best_model_scores[ndx][\"val_recall\"] for ndx in candidate_model_ndx]\n",
        "        if max(candidate_model_recall) <= min_recall:\n",
        "            logging.warning(f\"No models with recall >= {min_recall} found!\")\n",
        "            return None\n",
        "        else:\n",
        "            best_model = self.best_models[candidate_model_ndx[np.argmax(candidate_model_recall)]]\n",
        "            best_model_training_step = self.best_model_scores[candidate_model_ndx[np.argmax(candidate_model_recall)]][\"training_step_ndx\"]\n",
        "            logging.info(f\"Best model from training step {best_model_training_step} out of {len(candidate_model_ndx)}\"\n",
        "                         f\"models has recall of {np.max(candidate_model_recall)} and false positive rate of\"\n",
        "                         f\" {false_positive_rates[candidate_model_ndx[np.argmax(candidate_model_recall)]]}\")\n",
        "\n",
        "        return best_model\n",
        "\n",
        "    def auto_train(self, X_train, X_val, false_positive_val_data, steps=50000, max_negative_weight=1000,\n",
        "                   target_fp_per_hour=0.2):\n",
        "        \"\"\"A sequence of training steps that produce relatively strong models\n",
        "        automatically, based on validation data and performance targets provided.\n",
        "        After training merges the best checkpoints and returns a single model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get false positive validation data duration\n",
        "        val_set_hrs = 11.3\n",
        "\n",
        "        # Sequence 1\n",
        "        logging.info(\"#\"*50 + \"\\\\nStarting training sequence 1...\\\\n\" + \"#\"*50)\n",
        "        lr = 0.0001\n",
        "        weights = np.linspace(1, max_negative_weight, int(steps)).tolist()\n",
        "        val_steps = np.linspace(steps-int(steps*0.25), steps, 20).astype(np.int64)\n",
        "        self.train_model(\n",
        "                    X=X_train,\n",
        "                    X_val=X_val,\n",
        "                    false_positive_val_data=false_positive_val_data,\n",
        "                    max_steps=steps,\n",
        "                    negative_weight_schedule=weights,\n",
        "                    val_steps=val_steps, warmup_steps=steps//5,\n",
        "                    hold_steps=steps//3, lr=lr, val_set_hrs=val_set_hrs)\n",
        "\n",
        "        # Sequence 2\n",
        "        logging.info(\"#\"*50 + \"\\\\nStarting training sequence 2...\\\\n\" + \"#\"*50)\n",
        "        lr = lr/10\n",
        "        steps = steps/10\n",
        "\n",
        "        # Adjust weights as needed based on false positive per hour performance from first sequence\n",
        "        if self.best_val_fp > target_fp_per_hour:\n",
        "            max_negative_weight = max_negative_weight*2\n",
        "            logging.info(\"Increasing weight on negative examples to reduce false positives...\")\n",
        "\n",
        "        weights = np.linspace(1, max_negative_weight, int(steps)).tolist()\n",
        "        val_steps = np.linspace(1, steps, 20).astype(np.int16)\n",
        "        self.train_model(\n",
        "                    X=X_train,\n",
        "                    X_val=X_val,\n",
        "                    false_positive_val_data=false_positive_val_data,\n",
        "                    max_steps=steps,\n",
        "                    negative_weight_schedule=weights,\n",
        "                    val_steps=val_steps, warmup_steps=steps//5,\n",
        "                    hold_steps=steps//3, lr=lr, val_set_hrs=val_set_hrs)\n",
        "\n",
        "        # Sequence 3\n",
        "        logging.info(\"#\"*50 + \"\\\\nStarting training sequence 3...\\\\n\" + \"#\"*50)\n",
        "        lr = lr/10\n",
        "\n",
        "        # Adjust weights as needed based on false positive per hour performance from second sequence\n",
        "        if self.best_val_fp > target_fp_per_hour:\n",
        "            max_negative_weight = max_negative_weight*2\n",
        "            logging.info(\"Increasing weight on negative examples to reduce false positives...\")\n",
        "\n",
        "        weights = np.linspace(1, max_negative_weight, int(steps)).tolist()\n",
        "        val_steps = np.linspace(1, steps, 20).astype(np.int16)\n",
        "        self.train_model(\n",
        "                    X=X_train,\n",
        "                    X_val=X_val,\n",
        "                    false_positive_val_data=false_positive_val_data,\n",
        "                    max_steps=steps,\n",
        "                    negative_weight_schedule=weights,\n",
        "                    val_steps=val_steps, warmup_steps=steps//5,\n",
        "                    hold_steps=steps//3, lr=lr, val_set_hrs=val_set_hrs)\n",
        "\n",
        "        # Merge best models\n",
        "        logging.info(\"Merging checkpoints above the 90th percentile into single model...\")\n",
        "        accuracy_percentile = np.percentile(self.history[\"val_accuracy\"], 90)\n",
        "        recall_percentile = np.percentile(self.history[\"val_recall\"], 90)\n",
        "        fp_percentile = np.percentile(self.history[\"val_fp_per_hr\"], 10)\n",
        "\n",
        "        # Get models above the 90th percentile\n",
        "        models = []\n",
        "        for model, score in zip(self.best_models, self.best_model_scores):\n",
        "            if score[\"val_accuracy\"] >= accuracy_percentile and \\\n",
        "                    score[\"val_recall\"] >= recall_percentile and \\\n",
        "                    score[\"val_fp_per_hr\"] <= fp_percentile:\n",
        "                models.append(model)\n",
        "\n",
        "        if len(models) > 0:\n",
        "            combined_model = self.average_models(models=models)\n",
        "        else:\n",
        "            combined_model = self.model\n",
        "\n",
        "        # Report validation metrics for combined model\n",
        "        with torch.no_grad():\n",
        "            for batch in X_val:\n",
        "                x, y = batch[0].to(self.device), batch[1].to(self.device)\n",
        "                val_ps = combined_model(x)\n",
        "\n",
        "            combined_model_recall = self.recall(val_ps, y[..., None]).detach().cpu().numpy()\n",
        "            combined_model_accuracy = self.accuracy(val_ps, y[..., None].to(torch.int64)).detach().cpu().numpy()\n",
        "\n",
        "            combined_model_fp = 0\n",
        "            for batch in false_positive_val_data:\n",
        "                x_val, y_val = batch[0].to(self.device), batch[1].to(self.device)\n",
        "                val_ps = combined_model(x_val)\n",
        "                combined_model_fp += self.fp(val_ps, y_val[..., None])\n",
        "\n",
        "            combined_model_fp_per_hr = (combined_model_fp/val_set_hrs).detach().cpu().numpy()\n",
        "\n",
        "        logging.info(f\"\\\\n################\\\\nFinal Model Accuracy: {combined_model_accuracy}\"\n",
        "                     f\"\\\\nFinal Model Recall: {combined_model_recall}\\\\nFinal Model False Positives per Hour: {combined_model_fp_per_hr}\"\n",
        "                     \"\\\\n################\\\\n\")\n",
        "\n",
        "        return combined_model\n",
        "\n",
        "    def predict_on_features(self, features, model=None):\n",
        "        \"\"\"\n",
        "        Predict on Tensors of openWakeWord features corresponding to single audio clips\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): A Tensor of openWakeWord features with shape (batch, features)\n",
        "            model (torch.nn.Module): A Pytorch model to use for prediction (default None, which will use self.model)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: An array of predictions of shape (batch, prediction), where 0 is negative and 1 is positive\n",
        "        \"\"\"\n",
        "        if len(features) < 3:\n",
        "            features = features[None, ]\n",
        "\n",
        "        features = features.to(self.device)\n",
        "        predictions = []\n",
        "        for x in tqdm(features, desc=\"Predicting on clips\"):\n",
        "            x = x[None, ]\n",
        "            batch = []\n",
        "            for i in range(0, x.shape[1]-16, 1):  # step size of 1 (80 ms)\n",
        "                batch.append(x[:, i:i+16, :])\n",
        "            batch = torch.vstack(batch)\n",
        "            if model is None:\n",
        "                preds = self.model(batch)\n",
        "            else:\n",
        "                preds = model(batch)\n",
        "            predictions.append(preds.detach().cpu().numpy()[None, ])\n",
        "\n",
        "        return np.vstack(predictions)\n",
        "\n",
        "    def predict_on_clips(self, clips, model=None):\n",
        "        \"\"\"\n",
        "        Predict on Tensors of 16-bit 16 khz audio data\n",
        "\n",
        "        Args:\n",
        "            clips (np.ndarray): A Numpy array of audio clips with shape (batch, samples)\n",
        "            model (torch.nn.Module): A Pytorch model to use for prediction (default None, which will use self.model)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: An array of predictions of shape (batch, prediction), where 0 is negative and 1 is positive\n",
        "        \"\"\"\n",
        "\n",
        "        # Get features from clips\n",
        "        F = AudioFeatures(device='cpu', ncpu=4)\n",
        "        features = F.embed_clips(clips, batch_size=16)\n",
        "\n",
        "        # Predict on features\n",
        "        preds = self.predict_on_features(torch.from_numpy(features), model=model)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def export_model(self, model, model_name, output_dir):\n",
        "        \"\"\"Saves the trained openwakeword model to both onnx and tflite formats\"\"\"\n",
        "\n",
        "        if self.n_classes != 1:\n",
        "            raise ValueError(\"Exporting models to both onnx and tflite with more than one class is currently not supported! \"\n",
        "                             \"Use the `export_to_onnx` function instead.\")\n",
        "\n",
        "        # Save ONNX model\n",
        "        logging.info(f\"####\\\\nSaving ONNX mode as '{os.path.join(output_dir, model_name + '.onnx')}'\")\n",
        "        model_to_save = copy.deepcopy(model)\n",
        "        torch.onnx.export(model_to_save.to(\"cpu\"), torch.rand(self.input_shape)[None, ],\n",
        "                          os.path.join(output_dir, model_name + \".onnx\"), opset_version=13)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train_model(self, X, max_steps, warmup_steps, hold_steps, X_val=None,\n",
        "                    false_positive_val_data=None, positive_test_clips=None,\n",
        "                    negative_weight_schedule=[1],\n",
        "                    val_steps=[250], lr=0.0001, val_set_hrs=1):\n",
        "        # Move models and main class to target device\n",
        "        self.to(self.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Train model\n",
        "        accumulation_steps = 1\n",
        "        accumulated_samples = 0\n",
        "        accumulated_predictions = torch.Tensor([]).to(self.device)\n",
        "        accumulated_labels = torch.Tensor([]).to(self.device)\n",
        "        for step_ndx, data in tqdm(enumerate(X, 0), total=max_steps, desc=\"Training\"):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            x, y = data[0].to(self.device), data[1].to(self.device)\n",
        "            y_ = y[..., None].to(torch.float32)\n",
        "\n",
        "            # Update learning rates\n",
        "            for g in self.optimizer.param_groups:\n",
        "                g['lr'] = self.lr_warmup_cosine_decay(step_ndx, warmup_steps=warmup_steps, hold=hold_steps,\n",
        "                                                      total_steps=max_steps, target_lr=lr)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions for batch\n",
        "            predictions = self.model(x)\n",
        "\n",
        "            # Construct batch with only samples that have high loss\n",
        "            neg_high_loss = predictions[(y == 0) & (predictions.squeeze() >= 0.001)]  # thresholds were chosen arbitrarily but work well\n",
        "            pos_high_loss = predictions[(y == 1) & (predictions.squeeze() < 0.999)]\n",
        "            y = torch.cat((y[(y == 0) & (predictions.squeeze() >= 0.001)], y[(y == 1) & (predictions.squeeze() < 0.999)]))\n",
        "            y_ = y[..., None].to(torch.float32)\n",
        "            predictions = torch.cat((neg_high_loss, pos_high_loss))\n",
        "\n",
        "            # Set weights for batch\n",
        "            if len(negative_weight_schedule) == 1:\n",
        "                w = torch.ones(y.shape[0])*negative_weight_schedule[0]\n",
        "                pos_ndcs = y == 1\n",
        "                w[pos_ndcs] = 1\n",
        "                w = w[..., None]\n",
        "            else:\n",
        "                if self.n_classes == 1:\n",
        "                    w = torch.ones(y.shape[0])*negative_weight_schedule[step_ndx]\n",
        "                    pos_ndcs = y == 1\n",
        "                    w[pos_ndcs] = 1\n",
        "                    w = w[..., None]\n",
        "\n",
        "            if predictions.shape[0] != 0:\n",
        "                # Do backpropagation, with gradient accumulation if the batch-size after selecting high loss examples is too small\n",
        "                loss = self.loss(predictions, y_ if self.n_classes == 1 else y, w.to(self.device))\n",
        "                loss = loss/accumulation_steps\n",
        "                accumulated_samples += predictions.shape[0]\n",
        "\n",
        "                if predictions.shape[0] >= 128:\n",
        "                    accumulated_predictions = predictions\n",
        "                    accumulated_labels = y_\n",
        "                if accumulated_samples < 128:\n",
        "                    accumulation_steps += 1\n",
        "                    accumulated_predictions = torch.cat((accumulated_predictions, predictions))\n",
        "                    accumulated_labels = torch.cat((accumulated_labels, y_))\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    accumulation_steps = 1\n",
        "                    accumulated_samples = 0\n",
        "\n",
        "                    self.history[\"loss\"].append(loss.detach().cpu().numpy())\n",
        "\n",
        "                    # Compute training metrics and log them\n",
        "                    fp = self.fp(accumulated_predictions, accumulated_labels if self.n_classes == 1 else y)\n",
        "                    self.n_fp += fp\n",
        "                    self.history[\"recall\"].append(self.recall(accumulated_predictions, accumulated_labels).detach().cpu().numpy())\n",
        "\n",
        "                    accumulated_predictions = torch.Tensor([]).to(self.device)\n",
        "                    accumulated_labels = torch.Tensor([]).to(self.device)\n",
        "\n",
        "            # Run validation and log validation metrics\n",
        "            if step_ndx in val_steps and step_ndx > 1 and false_positive_val_data is not None:\n",
        "                # Get false positives per hour with false positive data\n",
        "                val_fp = 0\n",
        "                for val_step_ndx, data in enumerate(false_positive_val_data):\n",
        "                    with torch.no_grad():\n",
        "                        x_val, y_val = data[0].to(self.device), data[1].to(self.device)\n",
        "                        val_predictions = self.model(x_val)\n",
        "                        val_fp += self.fp(val_predictions, y_val[..., None])\n",
        "                val_fp_per_hr = (val_fp/val_set_hrs).detach().cpu().numpy()\n",
        "                self.history[\"val_fp_per_hr\"].append(val_fp_per_hr)\n",
        "\n",
        "            # Get recall on test clips\n",
        "            if step_ndx in val_steps and step_ndx > 1 and positive_test_clips is not None:\n",
        "                tp = 0\n",
        "                fn = 0\n",
        "                for val_step_ndx, data in enumerate(positive_test_clips):\n",
        "                    with torch.no_grad():\n",
        "                        x_val = data[0].to(self.device)\n",
        "                        batch = []\n",
        "                        for i in range(0, x_val.shape[1]-16, 1):\n",
        "                            batch.append(x_val[:, i:i+16, :])\n",
        "                        batch = torch.vstack(batch)\n",
        "                        preds = self.model(batch)\n",
        "                        if any(preds >= 0.5):\n",
        "                            tp += 1\n",
        "                        else:\n",
        "                            fn += 1\n",
        "                self.history[\"positive_test_clips_recall\"].append(tp/(tp + fn))\n",
        "\n",
        "            if step_ndx in val_steps and step_ndx > 1 and X_val is not None:\n",
        "                # Get metrics for balanced test examples of positive and negative clips\n",
        "                for val_step_ndx, data in enumerate(X_val):\n",
        "                    with torch.no_grad():\n",
        "                        x_val, y_val = data[0].to(self.device), data[1].to(self.device)\n",
        "                        val_predictions = self.model(x_val)\n",
        "                        val_recall = self.recall(val_predictions, y_val[..., None]).detach().cpu().numpy()\n",
        "                        val_acc = self.accuracy(val_predictions, y_val[..., None].to(torch.int64))\n",
        "                        val_fp = self.fp(val_predictions, y_val[..., None])\n",
        "                self.history[\"val_accuracy\"].append(val_acc.detach().cpu().numpy())\n",
        "                self.history[\"val_recall\"].append(val_recall)\n",
        "                self.history[\"val_n_fp\"].append(val_fp.detach().cpu().numpy())\n",
        "\n",
        "            # Save models with a validation score above/below the 90th percentile\n",
        "            # of the validation scores up to that point\n",
        "            if step_ndx in val_steps and step_ndx > 1:\n",
        "                if self.history[\"val_n_fp\"][-1] <= np.percentile(self.history[\"val_n_fp\"], 50) and \\\n",
        "                   self.history[\"val_recall\"][-1] >= np.percentile(self.history[\"val_recall\"], 5):\n",
        "                    # logging.info(\"Saving checkpoint with metrics >= to targets!\")\n",
        "                    self.best_models.append(copy.deepcopy(self.model))\n",
        "                    self.best_model_scores.append({\"training_step_ndx\": step_ndx, \"val_n_fp\": self.history[\"val_n_fp\"][-1],\n",
        "                                                   \"val_recall\": self.history[\"val_recall\"][-1],\n",
        "                                                   \"val_accuracy\": self.history[\"val_accuracy\"][-1],\n",
        "                                                   \"val_fp_per_hr\": self.history.get(\"val_fp_per_hr\", [0])[-1]})\n",
        "                    self.best_val_recall = self.history[\"val_recall\"][-1]\n",
        "                    self.best_val_accuracy = self.history[\"val_accuracy\"][-1]\n",
        "\n",
        "            if step_ndx == max_steps-1:\n",
        "                break\n",
        "\n",
        "\n",
        "# Separate function to convert onnx models to tflite format\n",
        "def convert_onnx_to_tflite(onnx_model_path, output_path):\n",
        "    \"\"\"Converts an ONNX version of an openwakeword model to the Tensorflow tflite format.\"\"\"\n",
        "    # imports\n",
        "    import onnx\n",
        "    from onnx_tf.backend import prepare\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Convert to tflite from onnx model\n",
        "    onnx_model = onnx.load(onnx_model_path)\n",
        "    tf_rep = prepare(onnx_model, device=\"CPU\")\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tf_rep.export_graph(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        logging.info(f\"####\\\\nSaving tflite mode to '{output_path}'\")\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "    return None\n",
        "\n",
        "import torchaudio\n",
        "def generate_and_resample_samples(**kwargs):\n",
        "    # Gnre les chantillons avec Piper\n",
        "    generate_samples.generate_samples_onnx(**kwargs)\n",
        "\n",
        "    # Charge et resample tous les fichiers gnrs\n",
        "    output_dir = kwargs.get(\"output_dir\")\n",
        "    target_sr = 16000\n",
        "\n",
        "    for wav_file in Path(output_dir).glob(\"*.wav\"):\n",
        "        audio, sr = torchaudio.load(wav_file)\n",
        "        if sr != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
        "            audio = resampler(audio)\n",
        "            torchaudio.save(wav_file, audio, target_sr)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Get training config file\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--training_config\",\n",
        "        help=\"The path to the training config file (required)\",\n",
        "        type=str,\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--generate_clips\",\n",
        "        help=\"Execute the synthetic data generation process\",\n",
        "        action=\"store_true\",\n",
        "        default=\"False\",\n",
        "        required=False\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--augment_clips\",\n",
        "        help=\"Execute the synthetic data augmentation process\",\n",
        "        action=\"store_true\",\n",
        "        default=\"False\",\n",
        "        required=False\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite\",\n",
        "        help=\"Overwrite existing openwakeword features when the --augment_clips flag is used\",\n",
        "        action=\"store_true\",\n",
        "        default=\"False\",\n",
        "        required=False\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_model\",\n",
        "        help=\"Execute the model training process\",\n",
        "        action=\"store_true\",\n",
        "        default=\"False\",\n",
        "        required=False\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    config = yaml.load(open(args.training_config, 'r').read(), yaml.Loader)\n",
        "\n",
        "    # imports Piper for synthetic sample generation\n",
        "    sys.path.insert(0, os.path.abspath(config[\"piper_sample_generator_path\"]))\n",
        "    import generate_samples\n",
        "\n",
        "    # Define output locations\n",
        "    config[\"output_dir\"] = os.path.abspath(config[\"output_dir\"])\n",
        "    if not os.path.exists(config[\"output_dir\"]):\n",
        "        os.mkdir(config[\"output_dir\"])\n",
        "    if not os.path.exists(os.path.join(config[\"output_dir\"], config[\"model_name\"])):\n",
        "        os.mkdir(os.path.join(config[\"output_dir\"], config[\"model_name\"]))\n",
        "\n",
        "    positive_train_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"positive_train\")\n",
        "    positive_test_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"positive_test\")\n",
        "    negative_train_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"negative_train\")\n",
        "    negative_test_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"negative_test\")\n",
        "    feature_save_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"])\n",
        "\n",
        "    # Get paths for impulse response and background audio files\n",
        "    rir_paths = [i.path for j in config[\"rir_paths\"] for i in os.scandir(j)]\n",
        "    background_paths = []\n",
        "    if len(config[\"background_paths_duplication_rate\"]) != len(config[\"background_paths\"]):\n",
        "        config[\"background_paths_duplication_rate\"] = [1]*len(config[\"background_paths\"])\n",
        "    for background_path, duplication_rate in zip(config[\"background_paths\"], config[\"background_paths_duplication_rate\"]):\n",
        "        background_paths.extend([i.path for i in os.scandir(background_path)]*duplication_rate)\n",
        "\n",
        "    if args.generate_clips is True:\n",
        "        # Generate positive clips for training\n",
        "        logging.info(\"#\"*50 + \"\\\\nGenerating positive clips for training\\\\n\" + \"#\"*50)\n",
        "        if not os.path.exists(positive_train_output_dir):\n",
        "            os.mkdir(positive_train_output_dir)\n",
        "        n_current_samples = len(os.listdir(positive_train_output_dir))\n",
        "        if n_current_samples <= 0.95*config[\"n_samples\"]:\n",
        "            generate_and_resample_samples(\n",
        "                text=config[\"target_phrase\"], max_samples=config[\"n_samples\"]-n_current_samples,\n",
        "                batch_size=config[\"tts_batch_size\"],\n",
        "                noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
        "                output_dir=positive_train_output_dir, auto_reduce_batch_size=True,\n",
        "                file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])],\n",
        "                model='./piper-sample-generator/models/fr_FR-upmc-medium.onnx'\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logging.warning(f\"Skipping generation of positive clips for training, as ~{config['n_samples']} already exist\")\n",
        "\n",
        "        # Generate positive clips for testing\n",
        "        logging.info(\"#\"*50 + \"\\\\nGenerating positive clips for testing\\\\n\" + \"#\"*50)\n",
        "        if not os.path.exists(positive_test_output_dir):\n",
        "            os.mkdir(positive_test_output_dir)\n",
        "        n_current_samples = len(os.listdir(positive_test_output_dir))\n",
        "        if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
        "            generate_and_resample_samples(text=config[\"target_phrase\"], max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
        "                             batch_size=config[\"tts_batch_size\"],\n",
        "                             noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
        "                             output_dir=positive_test_output_dir, auto_reduce_batch_size=True,\n",
        "                             model='./piper-sample-generator/models/fr_FR-upmc-medium.onnx'\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logging.warning(f\"Skipping generation of positive clips testing, as ~{config['n_samples_val']} already exist\")\n",
        "\n",
        "        # Generate adversarial negative clips for training\n",
        "        logging.info(\"#\"*50 + \"\\\\nGenerating negative clips for training\\\\n\" + \"#\"*50)\n",
        "        if not os.path.exists(negative_train_output_dir):\n",
        "            os.mkdir(negative_train_output_dir)\n",
        "        n_current_samples = len(os.listdir(negative_train_output_dir))\n",
        "        if n_current_samples <= 0.95*config[\"n_samples\"]:\n",
        "            adversarial_texts = config[\"custom_negative_phrases\"]\n",
        "            for target_phrase in config[\"target_phrase\"]:\n",
        "                adversarial_texts.extend(generate_adversarial_texts(\n",
        "                    input_text=target_phrase,\n",
        "                    N=config[\"n_samples\"]//len(config[\"target_phrase\"]),\n",
        "                    include_partial_phrase=1.0,\n",
        "                    include_input_words=0.2))\n",
        "            generate_and_resample_samples(text=adversarial_texts, max_samples=config[\"n_samples\"]-n_current_samples,\n",
        "                             batch_size=config[\"tts_batch_size\"]//7,\n",
        "                             noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
        "                             output_dir=negative_train_output_dir, auto_reduce_batch_size=True,\n",
        "                             file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])],\n",
        "                             model='./piper-sample-generator/models/fr_FR-upmc-medium.onnx'\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logging.warning(f\"Skipping generation of negative clips for training, as ~{config['n_samples']} already exist\")\n",
        "\n",
        "        # Generate adversarial negative clips for testing\n",
        "        logging.info(\"#\"*50 + \"\\\\nGenerating negative clips for testing\\\\n\" + \"#\"*50)\n",
        "        if not os.path.exists(negative_test_output_dir):\n",
        "            os.mkdir(negative_test_output_dir)\n",
        "        n_current_samples = len(os.listdir(negative_test_output_dir))\n",
        "        if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
        "            adversarial_texts = config[\"custom_negative_phrases\"]\n",
        "            for target_phrase in config[\"target_phrase\"]:\n",
        "                adversarial_texts.extend(generate_adversarial_texts(\n",
        "                    input_text=target_phrase,\n",
        "                    N=config[\"n_samples_val\"]//len(config[\"target_phrase\"]),\n",
        "                    include_partial_phrase=1.0,\n",
        "                    include_input_words=0.2))\n",
        "            generate_and_resample_samples(text=adversarial_texts, max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
        "                             batch_size=config[\"tts_batch_size\"]//7,\n",
        "                             noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
        "                             output_dir=negative_test_output_dir, auto_reduce_batch_size=True,\n",
        "                             model='./piper-sample-generator/models/fr_FR-upmc-medium.onnx'\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logging.warning(f\"Skipping generation of negative clips for testing, as ~{config['n_samples_val']} already exist\")\n",
        "\n",
        "    # Set the total length of the training clips based on the ~median generated clip duration, rounding to the nearest 1000 samples\n",
        "    # and setting to 32000 when the median + 750 ms is close to that, as it's a good default value\n",
        "    n = 50  # sample size\n",
        "    positive_clips = [str(i) for i in Path(positive_test_output_dir).glob(\"*.wav\")]\n",
        "    duration_in_samples = []\n",
        "    for i in range(n):\n",
        "        sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
        "        duration_in_samples.append(len(dat))\n",
        "\n",
        "    config[\"total_length\"] = int(round(np.median(duration_in_samples)/1000)*1000) + 12000  # add 750 ms to clip duration as buffer\n",
        "    if config[\"total_length\"] < 32000:\n",
        "        config[\"total_length\"] = 32000  # set a minimum of 32000 samples (2 seconds)\n",
        "    elif abs(config[\"total_length\"] - 32000) <= 4000:\n",
        "        config[\"total_length\"] = 32000\n",
        "\n",
        "    # Do Data Augmentation\n",
        "    if args.augment_clips is True:\n",
        "        if not os.path.exists(os.path.join(feature_save_dir, \"positive_features_train.npy\")) or args.overwrite is True:\n",
        "            positive_clips_train = [str(i) for i in Path(positive_train_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "            positive_clips_train_generator = augment_clips(positive_clips_train, total_length=config[\"total_length\"],\n",
        "                                                           batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                           background_clip_paths=background_paths,\n",
        "                                                           RIR_paths=rir_paths)\n",
        "\n",
        "            positive_clips_test = [str(i) for i in Path(positive_test_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "            positive_clips_test_generator = augment_clips(positive_clips_test, total_length=config[\"total_length\"],\n",
        "                                                          batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                          background_clip_paths=background_paths,\n",
        "                                                          RIR_paths=rir_paths)\n",
        "\n",
        "            negative_clips_train = [str(i) for i in Path(negative_train_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "            negative_clips_train_generator = augment_clips(negative_clips_train, total_length=config[\"total_length\"],\n",
        "                                                           batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                           background_clip_paths=background_paths,\n",
        "                                                           RIR_paths=rir_paths)\n",
        "\n",
        "            negative_clips_test = [str(i) for i in Path(negative_test_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "            negative_clips_test_generator = augment_clips(negative_clips_test, total_length=config[\"total_length\"],\n",
        "                                                          batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                          background_clip_paths=background_paths,\n",
        "                                                          RIR_paths=rir_paths)\n",
        "\n",
        "            # Compute features and save to disk via memmapped arrays\n",
        "            logging.info(\"#\"*50 + \"\\\\nComputing openwakeword features for generated samples\\\\n\" + \"#\"*50)\n",
        "            n_cpus = os.cpu_count()\n",
        "            if n_cpus is None:\n",
        "                n_cpus = 1\n",
        "            else:\n",
        "                n_cpus = n_cpus//2\n",
        "            compute_features_from_generator(positive_clips_train_generator, n_total=len(os.listdir(positive_train_output_dir)),\n",
        "                                            clip_duration=config[\"total_length\"],\n",
        "                                            output_file=os.path.join(feature_save_dir, \"positive_features_train.npy\"),\n",
        "                                            device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                            ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "\n",
        "            compute_features_from_generator(negative_clips_train_generator, n_total=len(os.listdir(negative_train_output_dir)),\n",
        "                                            clip_duration=config[\"total_length\"],\n",
        "                                            output_file=os.path.join(feature_save_dir, \"negative_features_train.npy\"),\n",
        "                                            device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                            ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "\n",
        "            compute_features_from_generator(positive_clips_test_generator, n_total=len(os.listdir(positive_test_output_dir)),\n",
        "                                            clip_duration=config[\"total_length\"],\n",
        "                                            output_file=os.path.join(feature_save_dir, \"positive_features_test.npy\"),\n",
        "                                            device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                            ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "\n",
        "            compute_features_from_generator(negative_clips_test_generator, n_total=len(os.listdir(negative_test_output_dir)),\n",
        "                                            clip_duration=config[\"total_length\"],\n",
        "                                            output_file=os.path.join(feature_save_dir, \"negative_features_test.npy\"),\n",
        "                                            device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                            ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "        else:\n",
        "            logging.warning(\"Openwakeword features already exist, skipping data augmentation and feature generation\")\n",
        "\n",
        "    # Create openwakeword model\n",
        "    if args.train_model is True:\n",
        "        F = openwakeword.utils.AudioFeatures(device='cpu')\n",
        "        input_shape = np.load(os.path.join(feature_save_dir, \"positive_features_test.npy\")).shape[1:]\n",
        "\n",
        "        oww = Model(n_classes=1, input_shape=input_shape, model_type=config[\"model_type\"],\n",
        "                    layer_dim=config[\"layer_size\"], seconds_per_example=1280*input_shape[0]/16000)\n",
        "\n",
        "        # Create data transform function for batch generation to handle differ clip lengths (todo: write tests for this)\n",
        "        def f(x, n=input_shape[0]):\n",
        "            \"\"\"Simple transformation function to ensure negative data is the appropriate shape for the model size\"\"\"\n",
        "            if n > x.shape[1] or n < x.shape[1]:\n",
        "                x = np.vstack(x)\n",
        "                new_batch = np.array([x[i:i+n, :] for i in range(0, x.shape[0]-n, n)])\n",
        "            else:\n",
        "                return x\n",
        "            return new_batch\n",
        "\n",
        "        # Create label transforms as needed for model (currently only supports binary classification models)\n",
        "        data_transforms = {key: f for key in config[\"feature_data_files\"].keys()}\n",
        "        label_transforms = {}\n",
        "        for key in [\"positive\"] + list(config[\"feature_data_files\"].keys()) + [\"adversarial_negative\"]:\n",
        "            if key == \"positive\":\n",
        "                label_transforms[key] = lambda x: [1 for i in x]\n",
        "            else:\n",
        "                label_transforms[key] = lambda x: [0 for i in x]\n",
        "\n",
        "        # Add generated positive and adversarial negative clips to the feature data files dictionary\n",
        "        config[\"feature_data_files\"]['positive'] = os.path.join(feature_save_dir, \"positive_features_train.npy\")\n",
        "        config[\"feature_data_files\"]['adversarial_negative'] = os.path.join(feature_save_dir, \"negative_features_train.npy\")\n",
        "\n",
        "        # Make PyTorch data loaders for training and validation data\n",
        "        batch_generator = mmap_batch_generator(\n",
        "            config[\"feature_data_files\"],\n",
        "            n_per_class=config[\"batch_n_per_class\"],\n",
        "            data_transform_funcs=data_transforms,\n",
        "            label_transform_funcs=label_transforms\n",
        "        )\n",
        "\n",
        "        class IterDataset(torch.utils.data.IterableDataset):\n",
        "            def __init__(self, generator):\n",
        "                self.generator = generator\n",
        "\n",
        "            def __iter__(self):\n",
        "                return self.generator\n",
        "\n",
        "        n_cpus = os.cpu_count()\n",
        "        if n_cpus is None:\n",
        "            n_cpus = 1\n",
        "        else:\n",
        "            n_cpus = n_cpus//2\n",
        "        X_train = torch.utils.data.DataLoader(IterDataset(batch_generator),\n",
        "                                              batch_size=None, num_workers=n_cpus, prefetch_factor=16)\n",
        "\n",
        "        X_val_fp = np.load(config[\"false_positive_validation_data_path\"])\n",
        "        X_val_fp = np.array([X_val_fp[i:i+input_shape[0]] for i in range(0, X_val_fp.shape[0]-input_shape[0], 1)])  # reshape to match model\n",
        "        X_val_fp_labels = np.zeros(X_val_fp.shape[0]).astype(np.float32)\n",
        "        X_val_fp = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(torch.from_numpy(X_val_fp), torch.from_numpy(X_val_fp_labels)),\n",
        "            batch_size=len(X_val_fp_labels)\n",
        "        )\n",
        "\n",
        "        X_val_pos = np.load(os.path.join(feature_save_dir, \"positive_features_test.npy\"))\n",
        "        X_val_neg = np.load(os.path.join(feature_save_dir, \"negative_features_test.npy\"))\n",
        "        labels = np.hstack((np.ones(X_val_pos.shape[0]), np.zeros(X_val_neg.shape[0]))).astype(np.float32)\n",
        "\n",
        "        X_val = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(\n",
        "                torch.from_numpy(np.vstack((X_val_pos, X_val_neg))),\n",
        "                torch.from_numpy(labels)\n",
        "                ),\n",
        "            batch_size=len(labels)\n",
        "        )\n",
        "\n",
        "        # Run auto training\n",
        "        best_model = oww.auto_train(\n",
        "            X_train=X_train,\n",
        "            X_val=X_val,\n",
        "            false_positive_val_data=X_val_fp,\n",
        "            steps=config[\"steps\"],\n",
        "            max_negative_weight=config[\"max_negative_weight\"],\n",
        "            target_fp_per_hour=config[\"target_false_positives_per_hour\"],\n",
        "        )\n",
        "\n",
        "        # Export the trained model to onnx\n",
        "        oww.export_model(model=best_model, model_name=config[\"model_name\"], output_dir=config[\"output_dir\"])\n",
        "\n",
        "        # Convert the model from onnx to tflite format\n",
        "        convert_onnx_to_tflite(os.path.join(config[\"output_dir\"], config[\"model_name\"] + \".onnx\"),\n",
        "                               os.path.join(config[\"output_dir\"], config[\"model_name\"] + \".tflite\"))\n",
        "\n",
        "'''\n",
        "train_path = \"openwakeword/openwakeword/train.py\"\n",
        "with open(train_path, \"w\") as f:\n",
        "    f.write(custom_train_script)"
      ],
      "metadata": {
        "id": "oWahyFO20_mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74aa368-e0cd-4f04-b772-cea70dfa2fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'openwakeword' already exists and is not an empty directory.\n",
            "Obtaining file:///content/openwakeword\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openwakeword\n",
            "  Building editable for openwakeword (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openwakeword: filename=openwakeword-0.6.0-0.editable-py3-none-any.whl size=17481 sha256=d1fe7ebab8f03c3478f5082ae1d3007461cbbff9adebb40e2ee9e6d9305bc7ee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-79l1fqe2/wheels/95/b4/6f/f887431fea7b3379ef42ac3594a92164114b83a95abb8b7969\n",
            "Successfully built openwakeword\n",
            "Installing collected packages: openwakeword\n",
            "  Attempting uninstall: openwakeword\n",
            "    Found existing installation: openwakeword 0.6.0\n",
            "    Uninstalling openwakeword-0.6.0:\n",
            "      Successfully uninstalled openwakeword-0.6.0\n",
            "Successfully installed openwakeword-0.6.0\n",
            "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Requirement already satisfied: onnx2tf in /usr/local/lib/python3.12/dist-packages (1.28.8)\n",
            "Requirement already satisfied: onnx==1.17.0 in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from onnx==1.17.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from onnx==1.17.0) (5.29.5)\n",
            "Requirement already satisfied: onnxruntime==1.18.1 in /usr/local/lib/python3.12/dist-packages (1.18.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (25.9.23)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.18.1) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime==1.18.1) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime==1.18.1) (1.3.0)\n",
            "Requirement already satisfied: mutagen==1.47.0 in /usr/local/lib/python3.12/dist-packages (1.47.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 3. Train the Model\n",
        "# @markdown Now that you have verified your target wake word and downloaded the data,\n",
        "# @markdown the last step is to adjust the training paramaters (or keep\n",
        "# @markdown the defaults below) and start the training!\n",
        "\n",
        "# @markdown Each paramater controls a different aspect of training:\n",
        "# @markdown - `number_of_examples` controls how many examples of your wakeword\n",
        "# @markdown are generated. The default (1,000) usually produces a good model,\n",
        "# @markdown but between 30,000 and 50,000 is often the best.\n",
        "\n",
        "# @markdown - `number_of_training_steps` controls how long to train the model.\n",
        "# @markdown Similar to the number of examples, the default (10,000) usually works well\n",
        "# @markdown but training longer usually helps.\n",
        "\n",
        "# @markdown - `false_activation_penalty` controls how strongly false activations\n",
        "# @markdown are penalized during the training process. Higher values can make the model\n",
        "# @markdown much less likely to activate when it shouldn't, but may also cause it\n",
        "# @markdown to not activate when the wake word isn't spoken clearly and there is\n",
        "# @markdown background noise.\n",
        "\n",
        "# @markdown With the default values shown below,\n",
        "# @markdown this takes about 30 - 60 minutes total on the normal CPU Colab runtime.\n",
        "# @markdown If you want to train on more examples or train for longer,\n",
        "# @markdown try changing the runtime type to a GPU to significantly speedup\n",
        "# @markdown the example generating and model training.\n",
        "\n",
        "# @markdown When the model finishes training, you can navigate to the `my_custom_model` folder\n",
        "# @markdown in the file browser on the left (click on the folder icon), and download\n",
        "# @markdown the [your target wake word].onnx or  <your target wake word>.tflite files.\n",
        "# @markdown You can then use these as you would any other openWakeWord model!\n",
        "\n",
        "# Load default YAML config file for training\n",
        "import yaml\n",
        "config = yaml.load(open(\"openwakeword/examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
        "!pip install tensorflow==2.19.0\n",
        "!pip install onnx2tf\n",
        "!pip install onnx==1.17.0\n",
        "!pip install onnxruntime==1.18.1\n",
        "# Modify values in the config and save a new version\n",
        "number_of_examples = 10500 # @param {type:\"slider\", min:100, max:50000, step:50}\n",
        "number_of_training_steps = 10000  # @param {type:\"slider\", min:0, max:50000, step:100}\n",
        "false_activation_penalty = 1500  # @param {type:\"slider\", min:100, max:5000, step:50}\n",
        "config[\"target_phrase\"] = [target_word]\n",
        "config[\"model_name\"] = config[\"target_phrase\"][0].replace(\" \", \"_\")\n",
        "config[\"n_samples\"] = number_of_examples\n",
        "config[\"n_samples_val\"] = max(500, number_of_examples//10)\n",
        "config[\"steps\"] = number_of_training_steps\n",
        "config[\"target_accuracy\"] = 0.5\n",
        "config[\"target_recall\"] = 0.25\n",
        "config[\"output_dir\"] = \"./my_custom_model\"\n",
        "config[\"max_negative_weight\"] = false_activation_penalty\n",
        "\n",
        "config[\"background_paths\"] = ['./audioset_16k', './fma']  # multiple background datasets are supported\n",
        "config[\"false_positive_validation_data_path\"] = \"validation_set_features.npy\"\n",
        "config[\"feature_data_files\"] = {\"ACAV100M_sample\": \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"}\n",
        "\n",
        "with open('my_model.yaml', 'w') as file:\n",
        "    documents = yaml.dump(config, file)\n",
        "\n",
        "# Generate clips\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips\n",
        "\n",
        "# Step 2: Augment the generated clips\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips\n",
        "\n",
        "# Step 3: Train model\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model\n",
        "\n",
        "# # Manually save to tflite as this doesn't work right in colab (broken in python 3.11, default in Colab as of January 2025)\n",
        "# def convert_onnx_to_tflite(onnx_model_path, output_path):\n",
        "#     \"\"\"Converts an ONNX version of an openwakeword model to the Tensorflow tflite format.\"\"\"\n",
        "#     # imports\n",
        "#     import onnx\n",
        "#     import logging\n",
        "#     import tempfile\n",
        "#     from onnx_tf.backend import prepare\n",
        "#     import tensorflow as tf\n",
        "\n",
        "#     # Convert to tflite from onnx model\n",
        "#     onnx_model = onnx.load(onnx_model_path)\n",
        "#     tf_rep = prepare(onnx_model, device=\"CPU\")\n",
        "#     with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "#         tf_rep.export_graph(os.path.join(tmp_dir, \"tf_model\"))\n",
        "#         converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, \"tf_model\"))\n",
        "#         tflite_model = converter.convert()\n",
        "\n",
        "#         logging.info(f\"####\\nSaving tflite mode to '{output_path}'\")\n",
        "#         with open(output_path, 'wb') as f:\n",
        "#             f.write(tflite_model)\n",
        "\n",
        "#     return None\n",
        "\n",
        "# convert_onnx_to_tflite(f\"my_custom_model/{config['model_name']}.onnx\", f\"my_custom_model/{config['model_name']}.tflite\")\n",
        "\n",
        "# Convert ONNX model to tflite using `onnx2tf` library (works for python 3.11 as of January 2025)\n",
        "onnx_model_path = f\"my_custom_model/{config['model_name']}.onnx\"\n",
        "name1, name2 = f\"my_custom_model/{config['model_name']}_float32.tflite\", f\"my_custom_model/{config['model_name']}.tflite\"\n",
        "!onnx2tf -i {onnx_model_path} -o my_custom_model/ -kat onnx____Flatten_0\n",
        "!mv {name1} {name2}\n",
        "\n",
        "# Automatically download the trained model files\n",
        "from google.colab import files\n",
        "\n",
        "files.download(f\"my_custom_model/{config['model_name']}.onnx\")\n",
        "files.download(f\"my_custom_model/{config['model_name']}.tflite\")\n"
      ],
      "metadata": {
        "id": "qgaKWIY6WlJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "90041911-f5d0-44c3-e251-9045b6d8ff46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'openwakeword/examples/custom_model.yml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4283378647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Load default YAML config file for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openwakeword/examples/custom_model.yml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow==2.19.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install onnx2tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'openwakeword/examples/custom_model.yml'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "nom_ton_fichier = \"nexus.mp3\"  # <--- CHANGE LE NOM SI BESOIN\n",
        "# ---------------------\n",
        "\n",
        "# 1. Nettoyage et cration du dossier\n",
        "target_dir = \"./target_word_samples\"\n",
        "if os.path.exists(target_dir):\n",
        "    shutil.rmtree(target_dir)\n",
        "os.makedirs(target_dir)\n",
        "\n",
        "# 2. Conversion MP3 -> WAV 16kHz Mono\n",
        "if os.path.exists(nom_ton_fichier):\n",
        "    print(f\" Conversion de {nom_ton_fichier}...\")\n",
        "    # librosa gre le MP3 nativement et convertit en 16000Hz automatiquement\n",
        "    y, sr = librosa.load(nom_ton_fichier, sr=16000, mono=True)\n",
        "\n",
        "    # On gnre 20 copies pour que l'tape d'augmentation ait assez de matire\n",
        "    for i in range(20):\n",
        "        sf.write(f\"{target_dir}/nexus_eleven_{i}.wav\", y, 16000, subtype='PCM_16')\n",
        "\n",
        "    print(f\" Termin ! 20 chantillons fluides sont dans {target_dir}\")\n",
        "else:\n",
        "    print(f\" Erreur : Je ne vois pas le fichier '{nom_ton_fichier}' dans Colab.\")"
      ],
      "metadata": {
        "id": "ozS7UhNY9XR7",
        "outputId": "985d517c-57b8-4d45-da30-1e4d2b1b4a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Conversion de nexus.mp3...\n",
            " Termin ! 20 chantillons fluides sont dans ./target_word_samples\n"
          ]
        }
      ]
    }
  ]
}